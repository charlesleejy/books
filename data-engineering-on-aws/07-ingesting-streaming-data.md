## Chapter 7: Ingesting Streaming Data

#### Overview
- This chapter focuses on the ingestion and processing of real-time streaming data using AWS services.
- It covers various AWS tools and techniques for capturing, processing, and analyzing streaming data.

### Key Concepts

1. **Streaming Data**
   - **Definition:** Continuous flow of data generated by various sources such as IoT devices, application logs, social media feeds, and transaction logs.
   - **Characteristics:** High volume, high velocity, time-sensitive, often unstructured.

2. **Use Cases**
   - Real-time analytics and dashboards
   - Monitoring and alerting systems
   - Fraud detection and prevention
   - Log and event data processing
   - Real-time recommendation systems

### AWS Services for Streaming Data

1. **Amazon Kinesis**
   - **Kinesis Data Streams:** Real-time data streaming service for building custom applications that process or analyze streaming data.
   - **Kinesis Data Firehose:** Fully managed service for reliably loading streaming data into data lakes, data stores, and analytics services.
   - **Kinesis Data Analytics:** Real-time stream processing and SQL querying of streaming data.
   - **Kinesis Video Streams:** Service for securely streaming video data.

   **Example:**
   ```python
   import boto3

   kinesis = boto3.client('kinesis')
   response = kinesis.put_record(
       StreamName='my-stream',
       Data=b'{"event":"test"}',
       PartitionKey='partition-key'
   )
   ```

2. **Amazon MSK (Managed Streaming for Apache Kafka)**
   - **Purpose:** Fully managed service for building and running applications that use Apache Kafka to process streaming data.
   - **Features:** Simplifies setup and maintenance of Kafka clusters, integrates with other AWS services.
   - **Use Cases:** Real-time data pipelines, log aggregation, event sourcing.

   **Example:**
   ```python
   import boto3

   msk = boto3.client('kafka')
   response = msk.create_cluster(
       ClusterName='my-kafka-cluster',
       KafkaVersion='2.6.0',
       NumberOfBrokerNodes=3,
       BrokerNodeGroupInfo={
           'InstanceType': 'kafka.m5.large',
           'ClientSubnets': ['subnet-12345678'],
           'SecurityGroups': ['sg-12345678']
       }
   )
   ```

3. **AWS Lambda**
   - **Purpose:** Serverless compute service that allows you to run code in response to events, such as data streams.
   - **Features:** Automatic scaling, pay-per-use, integration with various AWS services.
   - **Use Cases:** Data transformation, event-driven processing, microservices.

   **Example:**
   ```python
   import boto3

   lambda_client = boto3.client('lambda')
   response = lambda_client.create_function(
       FunctionName='process-streaming-data',
       Runtime='python3.8',
       Role='arn:aws:iam::123456789012:role/lambda-ex',
       Handler='lambda_function.lambda_handler',
       Code={
           'ZipFile': open('function.zip', 'rb').read()
       }
   )
   ```

### Setting Up Streaming Data Pipelines

1. **Ingesting Data with Kinesis Data Streams**
   - **Steps:**
     - Create a Kinesis data stream.
     - Write data to the stream using AWS SDKs or Kinesis Agent.
     - Process the data using consumers like AWS Lambda or Kinesis Data Analytics.

   **Example:**
   ```python
   import boto3

   kinesis = boto3.client('kinesis')
   response = kinesis.create_stream(
       StreamName='my-data-stream',
       ShardCount=1
   )
   ```

2. **Loading Data with Kinesis Data Firehose**
   - **Steps:**
     - Create a Firehose delivery stream.
     - Configure the destination (e.g., Amazon S3, Amazon Redshift, Amazon Elasticsearch Service).
     - Optionally, configure data transformation using AWS Lambda.

   **Example:**
   ```python
   import boto3

   firehose = boto3.client('firehose')
   response = firehose.create_delivery_stream(
       DeliveryStreamName='my-delivery-stream',
       S3DestinationConfiguration={
           'RoleARN': 'arn:aws:iam::123456789012:role/firehose_delivery_role',
           'BucketARN': 'arn:aws:s3:::my-bucket',
           'Prefix': 'firehose/'
       }
   )
   ```

3. **Real-Time Analytics with Kinesis Data Analytics**
   - **Steps:**
     - Create a Kinesis data analytics application.
     - Define the input source (Kinesis data stream or Firehose delivery stream).
     - Write SQL queries to process and analyze the streaming data.
     - Define the output destination (e.g., another Kinesis data stream, Firehose, or S3).

   **Example:**
   ```python
   import boto3

   kinesis_analytics = boto3.client('kinesisanalytics')
   response = kinesis_analytics.create_application(
       ApplicationName='my-kinesis-analytics-app',
       Inputs=[
           {
               'NamePrefix': 'source-stream',
               'KinesisStreamsInput': {
                   'ResourceARN': 'arn:aws:kinesis:us-east-1:123456789012:stream/my-data-stream',
                   'RoleARN': 'arn:aws:iam::123456789012:role/kinesis-analytics-role'
               },
               'InputSchema': {
                   'RecordFormat': {
                       'RecordFormatType': 'JSON',
                       'MappingParameters': {
                           'JSONMappingParameters': {
                               'RecordRowPath': '$'
                           }
                       }
                   },
                   'RecordColumns': [
                       {
                           'Name': 'event',
                           'SqlType': 'VARCHAR(64)',
                           'Mapping': '$.event'
                       }
                   ]
               }
           }
       ]
   )
   ```

### Processing and Transforming Streaming Data

1. **Data Transformation with AWS Lambda**
   - **Steps:**
     - Create a Lambda function to process streaming data.
     - Configure Kinesis or Firehose to trigger the Lambda function.
     - Implement the transformation logic in the Lambda function.

   **Example:**
   ```python
   import json

   def lambda_handler(event, context):
       for record in event['Records']:
           payload = json.loads(record['kinesis']['data'])
           print(f"Decoded payload: {payload}")
   ```

2. **Using AWS Glue for Streaming ETL**
   - **Steps:**
     - Create an AWS Glue streaming ETL job.
     - Define the input source (Kinesis data stream or Kafka topic).
     - Define the transformation logic using Glue's Spark environment.
     - Define the output destination (e.g., Amazon S3, Redshift).

   **Example:**
   ```python
   import boto3

   glue = boto3.client('glue')
   response = glue.create_job(
       Name='streaming-etl-job',
       Role='arn:aws:iam::123456789012:role/glue-service-role',
       Command={
           'Name': 'gluestreaming',
           'ScriptLocation': 's3://my-scripts/streaming-etl.py'
       }
   )
   ```

### Monitoring and Scaling Streaming Data Applications

1. **Amazon CloudWatch**
   - **Purpose:** Monitor and log AWS resources and applications.
   - **Features:** Metrics, logs, alarms, dashboards.
   - **Best Practices:** Set up custom metrics, create dashboards for visual monitoring, configure alarms for critical events.

   **Example:**
   ```python
   import boto3

   cloudwatch = boto3.client('cloudwatch')
   response = cloudwatch.put_metric_alarm(
       AlarmName='HighKinesisLatency',
       MetricName='GetRecords.IteratorAgeMilliseconds',
       Namespace='AWS/Kinesis',
       Statistic='Average',
       Period=300,
       Threshold=1000,
       ComparisonOperator='GreaterThanThreshold',
       Dimensions=[
           {'Name': 'StreamName', 'Value': 'my-data-stream'}
       ],
       EvaluationPeriods=1
   )
   ```

2. **Auto Scaling with Kinesis Data Streams**
   - **Purpose:** Automatically adjust the number of shards in a Kinesis stream based on the volume of incoming data.
   - **Best Practices:** Use application auto-scaling policies to maintain performance and cost efficiency.

   **Example:**
   ```python
   import boto3

   application_autoscaling = boto3.client('application-autoscaling')
   response = application_autoscaling.register_scalable_target(
       ServiceNamespace='kinesis',
       ResourceId='stream/my-data-stream',
       ScalableDimension='kinesis:stream:WriteCapacityUnits',
       MinCapacity=1,
       MaxCapacity=10,
       RoleARN='arn:aws:iam::123456789012:role/kinesis-autoscaling-role'
   )
   ```

### Conclusion
- Ingesting streaming data involves setting up data pipelines that can handle continuous data flow and real-time processing.
- AWS provides a suite of services like Amazon Kinesis, Amazon MSK, and AWS Lambda to build and manage these pipelines efficiently.
- Following best practices for data transformation, monitoring, and scaling ensures that streaming data applications are reliable, scalable, and performant.

These detailed notes provide a comprehensive overview of the key concepts and best practices covered in Chapter 7 of "Data Engineering with AWS" by Gareth Eagar. For more practical examples and in-depth explanations, refer to the book directly.